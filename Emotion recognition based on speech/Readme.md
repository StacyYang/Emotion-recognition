The **Interactive Emotional Dyadic Motion Capture (IEMO-CAP)** is used in this project, it is an acted, multimodal and multi speaker database. 
It contains approximately 12 hours of audiovisual data, including video, speech, motion capture of face, text transcriptions. 
In this project, we utilized speech audios which are dialogues and their corresponding text transcriptions.


The database is annotated by multiple annotators into categorical labels, such as anger, happiness, sadness, neutrality, as well as dimensional labels such as valence, activation and
dominance. Four emotion categories are kept, and the data sample number dropped from 10033 to 4488. 


### Steps
1. Data preparation<br>
   a) Extract labels for audio<br>
   b) Extract acoustic features from audio files<br>
2. Build Models<br>


### Future work
1. Try to use deep learning models<br>
2. Try to use multimodal method to combine audio and text features.<br>
